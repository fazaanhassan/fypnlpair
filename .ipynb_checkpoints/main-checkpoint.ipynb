{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "from lexicalrichness import LexicalRichness\n",
    "from sklearn.feature_extraction.text import CountVectorizer # LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation # LDA\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # NFM TD-IDF\n",
    "from sklearn.decomposition import NMF #NFM TD-IDF\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from datetime import datetime\n",
    "from prettytable import PrettyTable\n",
    "from langdetect import detect\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (61,62,94,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "reviews_file_main = pd.read_csv('dataset/reviews.csv', delimiter = ',')\n",
    "listings_file_main = pd.read_csv('dataset/listings.csv', delimiter = ',')\n",
    "listings_file_main_n = pd.read_csv('dataset/listingsN.csv', delimiter = ',')\n",
    "\n",
    "print(type(reviews_file_main['comments']))\n",
    "def get_reviews(segment):\n",
    "\n",
    "    reviews_file = reviews_file_main.head(segment)\n",
    "    reviews_file.dropna()\n",
    "    \n",
    "    return reviews_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iteration 1 - No Date sorted </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TTR Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews from top:  20\n",
      "Word count:  1254\n",
      "Unique terms:  459\n",
      "TTR:  0.3660287081339713\n",
      "\n",
      "Number of reviews from top:  200\n",
      "Word count:  14397\n",
      "Unique terms:  2442\n",
      "TTR:  0.1696186705563659\n",
      "\n",
      "Number of reviews from top:  2000\n",
      "Word count:  127752\n",
      "Unique terms:  8945\n",
      "TTR:  0.07001847329200325\n",
      "\n",
      "Number of reviews from top:  20000\n",
      "Word count:  1205299\n",
      "Unique terms:  33464\n",
      "TTR:  0.02776406518216642\n",
      "\n",
      "Number of reviews from top:  200000\n",
      "Word count:  11712352\n",
      "Unique terms:  130549\n",
      "TTR:  0.011146266778867302\n",
      "\n",
      "Number of reviews from top:  1000000\n",
      "Word count:  49519437\n",
      "Unique terms:  337872\n",
      "TTR:  0.006823017798041606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_ttr(segments):\n",
    "    \n",
    "    for i in segments:\n",
    "        reviews_file = get_reviews(i)\n",
    "        lex = LexicalRichness(\" \".join([str(comments) for comments in list(reviews_file['comments'])]))\n",
    "        \n",
    "        print(\"Number of reviews from top: \", i)\n",
    "        print(\"Word count: \",lex.words)\n",
    "        print(\"Unique terms: \",lex.terms)\n",
    "        print(\"TTR: \",lex.ttr)\n",
    "        print(\"\")\n",
    "    \n",
    "calculate_ttr([20, 200, 2000, 20000, 200000, 1000000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> LDA - Latent Dirichlet Allocation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(to_transform):\n",
    "    \n",
    "    count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vect.fit_transform(to_transform)\n",
    "\n",
    "\n",
    "    LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    LDA.fit(doc_term_matrix)\n",
    "\n",
    "    for i,topic in enumerate(LDA.components_):\n",
    "        print(f'Top 10 words for topic #{i}:')\n",
    "        print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "        print('\\n')\n",
    "        \n",
    "reviews_file = get_reviews(20000)\n",
    "lda(reviews_file['comments'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> NMF - Non-Negative Matrix Factorization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf(to_transform):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = tfidf_vect.fit_transform(to_transform)\n",
    "\n",
    "    nmf = NMF(n_components=5, random_state=42)\n",
    "    nmf.fit(doc_term_matrix )\n",
    "\n",
    "    for i,topic in enumerate(nmf.components_):\n",
    "        print(f'Top 10 words for topic #{i}:')\n",
    "        print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "        \n",
    "reviews_file = get_reviews(20000)        \n",
    "nmf(reviews_file['comments'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word Frequency </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wf(example_sent):\n",
    "    \n",
    "    reviews_file = get_reviews(example_sent[0])\n",
    "    from_reviews = \" \".join([str(comments) for comments in list(reviews_file['comments'])])\n",
    "        \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    result_tokens = tokenizer.tokenize(from_reviews)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rejoin = \" \".join(result_tokens)\n",
    "    word_tokens = word_tokenize(rejoin.lower()) \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    freq = list(dict(Counter(filtered_sentence)).items())\n",
    "    sorted_freq = sorted(freq, reverse = True, key = lambda x: x[1])\n",
    "\n",
    "    words = [word[0] for word in sorted_freq][0:10]\n",
    "    frequency = [freq[1] for freq in sorted_freq][0:10]\n",
    "    print(sorted_freq[0:10])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    plt.bar(words, frequency)\n",
    "    plt.xlabel('Word', fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel('#Occurrences', fontsize=12, fontweight=\"bold\")\n",
    "    plt.title('WC for reviews unsorted', fontsize = 16, fontweight=\"bold\")\n",
    "    for a,b in zip(words, frequency):\n",
    "        plt.text(a, b, str(b), horizontalalignment='center')\n",
    "    plt.show()\n",
    "\n",
    "find_wf([10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iteration 2 - Dates Sorted </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupled = [(date, review) for date, review in zip(list(reviews_file_main['date']), list(reviews_file_main['comments']))]\n",
    "coupled_sorted = sorted(coupled, reverse = False, key = lambda x: x[0])\n",
    "\n",
    "just_reviews_series = pd.Series([review[1] for review in coupled_sorted])\n",
    "traversed_dates = [datetime(year, 1, 1) for year in range(2010,2020)]\n",
    "date_positioning = []\n",
    "date_positioning_2 = [1, 169, 1477, 7866, 26564, 68292, 160406, 340463, 624425, 1068793]\n",
    "\n",
    "counter = 0\n",
    "for each_date in traversed_dates:\n",
    "    for each_review_date in coupled_sorted[counter:]:\n",
    "        if each_date > datetime(*map(int, each_review_date[0].split('-'))):\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "    date_positioning.append(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TTR #1 Cumulative  </h3>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_just_reviews(segment):\n",
    "\n",
    "    reviews_file = just_reviews_series.head(segment)\n",
    "    reviews_file.dropna()\n",
    "    \n",
    "    return reviews_file\n",
    "\n",
    "def calculate_ttr(segments):\n",
    "    \n",
    "    ttr_values = []\n",
    "    ttr_values_2 = [\n",
    "                    0.8571428571428571, 0.15486725663716813, \n",
    "                    0.06144981861030442, 0.03117285264322806, \n",
    "                    0.018954199271226692, 0.0135989986314741,\n",
    "                    0.010474976886623027, 0.008224299190462135,\n",
    "                    0.007261969645840463, 0.0065978822898372985\n",
    "                   ]\n",
    "    ttr_values_no_date = [\n",
    "                          0.3660287081339713, 0.1696186705563659, \n",
    "                          0.07001847329200325,0.02776406518216642, \n",
    "                          0.011146266778867302, 0.006823017798041606  \n",
    "                         ]\n",
    "    unique_terms = []\n",
    "    unique_terms_2 = [30, 2030, 6623, 17577, 36084, 65425, 113096, 172115, 244897, 344198]\n",
    "                \n",
    "    \n",
    "#     for i in segments[0:8]:\n",
    "#         reviews_file = get_just_reviews(i)\n",
    "#         lex = LexicalRichness(\" \".join([str(comments) for comments in list(reviews_file)]))\n",
    "        \n",
    "#         print(\"Number of reviews: \", i)\n",
    "#         print(\"Word count: \",lex.words)\n",
    "        \n",
    "#         lex_terms = lex.terms\n",
    "#         print(\"Unique terms: \",lex_terms)\n",
    "#         unique_terms.append(lex_terms)\n",
    "        \n",
    "#         lex_ttr = lex.ttr\n",
    "#         print(\"TTR: \",lex_ttr)\n",
    "#         ttr_values.append(lex_ttr)\n",
    "#         print(\"\")\n",
    "    \n",
    "    \n",
    "    my_dates = dates.date2num(traversed_dates)\n",
    "    date_equiv = [20, 200, 2000, 20000, 200000, 1000000]\n",
    "    \n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.xlabel('Date id', fontsize=12, fontweight = 'bold')\n",
    "    plt.ylabel('TRR', fontsize=12, fontweight = 'bold')\n",
    "    plt.title('TRR rates 2010-2019', fontsize=14, fontweight=\"bold\")\n",
    "    plt.plot_date(my_dates, ttr_values_2, ls = \"solid\", color = \"orange\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.xlabel('#Reviews', fontsize=12, fontweight = 'bold')\n",
    "    plt.ylabel('TRR', fontsize=12, fontweight = 'bold')\n",
    "    plt.title('TRR Overtime + #Reviews', fontsize=14, fontweight=\"bold\")\n",
    "    plt.plot(date_positioning_2, ttr_values_2, ls = \"dashdot\", color = \"orange\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.xlabel('#Reviews', fontsize=12, fontweight = 'bold')\n",
    "    plt.ylabel('TRR', fontsize=12, fontweight = 'bold')\n",
    "    plt.title('TRR on #Reviews', fontsize=14, fontweight=\"bold\")\n",
    "    plt.plot(date_equiv, ttr_values_no_date, ls = \"dashdot\")\n",
    "\n",
    "    plt.tight_layout() \n",
    "    \n",
    "    plt.figure(figsize=(10.2,5))\n",
    "    plt.xlabel('Date id', fontsize=17, fontweight=\"bold\")\n",
    "    plt.ylabel('Number of Terms', fontsize=17, fontweight=\"bold\")\n",
    "    plt.title('Unique Terms Rates 2010-2019', fontsize=20, fontweight=\"bold\")\n",
    "    plt.plot_date(my_dates, unique_terms_2, ls = \"solid\")\n",
    "    \n",
    "    \n",
    "calculate_ttr(date_positioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TTR #2 Each Year </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_just_reviews_yearly(segment):\n",
    "    reviews_file = just_reviews_series[segment[1]:segment[0]]\n",
    "    reviews_file.dropna()\n",
    "    \n",
    "    return reviews_file\n",
    "\n",
    "def calculate_ttr_yearly(segments):\n",
    "    \n",
    "    unique_terms = []\n",
    "    ttr_values = []\n",
    "    sliced_values=[0,0]\n",
    "    \n",
    "    x = PrettyTable()\n",
    "    x.field_names = ['year', 'Number of Reviews', 'Number of Words', 'Unique Terms', 'TTR']\n",
    "    start = \"2010\"\n",
    "    for counter, current in enumerate(segments):\n",
    "        \n",
    "        if counter != 0:\n",
    "            sliced_values[0] = current\n",
    "            sliced_values[1] = segments[counter - 1]\n",
    "            reviews_file = get_just_reviews_yearly(sliced_values)\n",
    "            lex = LexicalRichness(\" \".join([str(comments) for comments in list(reviews_file)]))\n",
    "\n",
    "            lex_terms = lex.terms\n",
    "            unique_terms.append(lex_terms)\n",
    "\n",
    "            lex_ttr = lex.ttr\n",
    "            ttr_values.append(lex_ttr)\n",
    "            \n",
    "            x.add_row([int(start) + counter, current-segments[counter-1], lex.words, lex_terms, lex_ttr])\n",
    "    print(x)\n",
    "    my_dates = dates.date2num(traversed_dates)\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.xlabel('Date id', fontsize=12, fontweight = 'bold')\n",
    "    plt.ylabel('TRR', fontsize=12, fontweight = 'bold')\n",
    "    plt.title('TRR rates 2010-2019', fontsize=14, fontweight=\"bold\")\n",
    "    plt.plot_date(my_dates[1:], ttr_values, ls = \"solid\", color = \"orange\")\n",
    "\n",
    "calculate_ttr_yearly(date_positioning_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> WC sorted by date </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wf(example_sent):\n",
    "    \n",
    "    for i, position in enumerate(example_sent):\n",
    "        reviews_file = get_just_reviews(position)\n",
    "        from_reviews = \" \".join([str(comments) for comments in list(reviews_file)])\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        result_tokens = tokenizer.tokenize(from_reviews)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        rejoin = \" \".join(result_tokens)\n",
    "        word_tokens = word_tokenize(rejoin.lower()) \n",
    "\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        freq = list(dict(Counter(filtered_sentence)).items())\n",
    "        sorted_freq = sorted(freq, reverse = True, key = lambda x: x[1])\n",
    "\n",
    "        words = [word[0] for word in sorted_freq][0:10]\n",
    "        frequency = [freq[1] for freq in sorted_freq][0:10]\n",
    "#         print(sorted_freq[0:10])\n",
    "    \n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.bar(words, frequency)\n",
    "        plt.xlabel('Word', fontsize=12, fontweight=\"bold\")\n",
    "        plt.ylabel('#Occurrences', fontsize=12, fontweight=\"bold\")\n",
    "        plt.title('WC for reviews sorted ' + str(traversed_dates[i+1])[0:11], fontsize = 16, fontweight=\"bold\")\n",
    "        for a,b in zip(words, frequency):\n",
    "            plt.text(a, b, str(b), horizontalalignment='center')\n",
    "        plt.show()\n",
    "\n",
    "# find_wf([169, 1477, 7866, 26564, 68292, 160406, 340463, 624425, 1068793])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TTR Listings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_ids = [15400,62970, 25123, 59656]\n",
    "ids = [\"EH - C\", \"EH - E\", \"PR - C\", \"PR - E\"]\n",
    "# [ EntireHome 85revs - £100/day, \n",
    "#   EntireHome 139revs - £350/day,\n",
    "#   PrivateRoom 122revs - £29/day, \n",
    "#   PrivateRoom 187revs - £75/day ]\n",
    "\n",
    "def calculate_ttr(coupled_sorted):\n",
    "\n",
    "    ttr_values = []\n",
    "    unique_terms = []\n",
    "    for counter, each_place in enumerate(coupled_sorted):\n",
    "        just_reviews_series = pd.Series([review[1] for review in each_place])\n",
    "        lex = LexicalRichness(\" \".join([str(comments) for comments in list(just_reviews_series)]))\n",
    "        \n",
    "#         print(\"Number of reviews: \", i)\n",
    "#         print(\"Word count: \",lex.words)\n",
    "        \n",
    "        lex_terms = lex.terms\n",
    "#         print(\"Unique terms: \",lex_terms)\n",
    "        unique_terms.append(lex_terms)\n",
    "        \n",
    "        lex_ttr = lex.ttr\n",
    "#         print(\"TTR: \",lex_ttr)\n",
    "        ttr_values.append(lex_ttr)\n",
    "        \n",
    "    plt.figure(figsize=(16,7))\n",
    "    plt.bar(ids, ttr_values)\n",
    "    plt.xlabel('Listing Id', fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel('TTR', fontsize=12, fontweight=\"bold\")\n",
    "    plt.title('TTR for Certain listings', fontsize = 16, fontweight=\"bold\")\n",
    "    for a,b in zip(ids, ttr_values):\n",
    "        plt.text(a, b, str(b), horizontalalignment='center')\n",
    "    plt.show()\n",
    "        \n",
    "def get_comments_id(id_place):\n",
    "    return [(date, review, listing_id) for date, review, listing_id in zip(list(reviews_file_main['date']),\\\n",
    "                                                                           list(reviews_file_main['comments']), list(reviews_file_main['listing_id'])) if listing_id == id_place]\n",
    "\n",
    "all_ids = [ get_comments_id(id_place) for id_place in room_ids]\n",
    "\n",
    "def create_corpus_ttr(all_id_places):\n",
    "\n",
    "    coupled_sorted = [sorted(id_place, reverse = False, key = lambda x: x[0]) for id_place in all_id_places]\n",
    "    calculate_ttr(coupled_sorted)\n",
    "    \n",
    "create_corpus_ttr(all_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> NMF Listings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_2(to_transform):\n",
    "    topic_table = PrettyTable()\n",
    "    topic_table.field_names = ['id', 'Words']\n",
    "    tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = tfidf_vect.fit_transform(to_transform)\n",
    "\n",
    "    nmf = NMF(n_components=5, random_state=42)\n",
    "    nmf.fit(doc_term_matrix )\n",
    "\n",
    "    for i,topic in enumerate(nmf.components_):\n",
    "        topic_table.add_row([\"Topic \" + str(i), [tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]]])\n",
    "    return topic_table\n",
    "\n",
    "counter = 0\n",
    "for id_place,which_id in zip(all_ids, room_ids):\n",
    "    print(\"#\"+str(which_id)+\"# \" + ids[counter])\n",
    "    each_place = sorted(id_place, reverse = False, key = lambda x: x[0])\n",
    "    just_reviews_series = pd.Series([review[1] for review in each_place])\n",
    "    topic_table = nmf_2(just_reviews_series.values.astype('U'))\n",
    "    print(topic_table)\n",
    "    print(\"\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iteration 3 - Properties Quadrant </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "location_and_ids = [(prop_id, neighbourhood, number_of_reviews) for prop_id, neighbourhood, number_of_reviews in zip(list(listings_file_main_n['id']), list(listings_file_main_n['neighbourhood']), list(listings_file_main_n['number_of_reviews']))]\n",
    "location_and_ids_dict = defaultdict(list)\n",
    "\n",
    "for prop_id, neighbourhood, number_of_reviews in location_and_ids:\n",
    "    location_and_ids_dict[neighbourhood].append((prop_id, number_of_reviews))\n",
    "\n",
    "possible_neighbourhoods = [each_loc for each_loc in location_and_ids_dict.keys()]\n",
    "\n",
    "all_properties = PrettyTable()\n",
    "all_properties.field_names = ['Area', 'Number of Properties', 'Number of Reviews']\n",
    "for each_loc, each_loc_id in location_and_ids_dict.items():\n",
    "    all_properties.add_row([each_loc, len(each_loc_id), sum(tuple2[1] for tuple2 in each_loc_id)]) \n",
    "print(all_properties)\n",
    "\n",
    "## Now create a dictionary such as ... {\"<location_name>\": [{<\"prop_id\"> : [(<date>,<review_prop_id1_1>)...()]}, {<\"prop_id2\">...}] \n",
    "location_and_properties_review = defaultdict(list)\n",
    "three_way = [(date, review, listing_id) for date, review, listing_id in zip(list(reviews_file_main['date']), list(reviews_file_main['comments']), list(reviews_file_main['listing_id']))]\n",
    " \n",
    "def calc_ttr(coupled_sorted, date_positioning, neighbourhood):\n",
    "    \n",
    "    unique_terms = []\n",
    "    ttr_values = []\n",
    "    sliced_values=[0, 0]\n",
    "    miss_zereos = 0\n",
    "    for counter, current in enumerate(date_positioning):\n",
    "        \n",
    "        if counter != 0 and current != 0:\n",
    "            sliced_values[0] = current\n",
    "            sliced_values[1] = date_positioning[counter - 1]\n",
    "            reviews_file = [str(date_review[1]) for date_review in coupled_sorted][sliced_values[1]:sliced_values[0]]\n",
    "            lex = LexicalRichness(\" \".join([str(comments) for comments in list(reviews_file)]))\n",
    "\n",
    "            lex_terms = lex.terms\n",
    "            unique_terms.append(lex_terms)\n",
    "\n",
    "            lex_ttr = lex.ttr\n",
    "            ttr_values.append(lex_ttr)\n",
    "        else:\n",
    "            miss_zereos += 1\n",
    "            \n",
    "    for each_miss in range(miss_zereos):\n",
    "        ttr_values.insert(0, 0)\n",
    "\n",
    "    my_dates = dates.date2num(traversed_dates)\n",
    "    plt.figure(figsize=(9.5,3.5))\n",
    "    plt.xlabel('Date id', fontsize=12, fontweight = 'bold')\n",
    "    plt.ylabel('TRR', fontsize=12, fontweight = 'bold')\n",
    "    plt.title(neighbourhood + ' TRR rates', fontsize=14, fontweight=\"bold\")\n",
    "    plt.plot_date(my_dates, ttr_values, ls = \"solid\", color = \"orange\")\n",
    "    \n",
    "def sort_by_date(big_string):\n",
    "    date_positioning = []\n",
    "    counter = 0\n",
    "    for each_date in traversed_dates:\n",
    "        for each_review_date in big_string[counter:]:\n",
    "            if each_date > datetime(*map(int, each_review_date[0].split('-'))):\n",
    "                counter += 1\n",
    "            else:\n",
    "                break\n",
    "        date_positioning.append(counter)\n",
    "    return date_positioning\n",
    "        \n",
    "def generate_string(neighbourhood):\n",
    "    big_string = []\n",
    "    current_prop_ids = [prop_id[0] for prop_id in location_and_ids_dict[neighbourhood]]\n",
    "    for three_way_tuple in three_way:\n",
    "        if three_way_tuple[2] in current_prop_ids:\n",
    "            big_string.append((three_way_tuple[0], three_way_tuple[1])) ## (date, review) as a tuple\n",
    "    \n",
    "    \n",
    "    create_sorted_reviews = sorted(big_string, reverse = False, key = lambda x: x[0])\n",
    "    just_reviews_series = pd.Series([str(review[1]) for review in create_sorted_reviews])\n",
    "    return just_reviews_series\n",
    "\n",
    "#     date_pos = sort_by_date(create_sorted_reviews)\n",
    "#     calc_ttr(create_sorted_reviews, date_pos, neighbourhood)\n",
    "    \n",
    "\n",
    "four_areas_ttr2 = [[0, 0, 0, 0.28433268858800775, 0.24594155844155843, 0.1921589127025614, 0.13599448737510458, 0.1138782751091703, 0.09587072901905212, 0.08026600680482524], [0, 0, 0, 0.28619302949061665, 0.2299028579473462, 0.2044912990107589, 0.1456943366951125, 0.10620770178814677, 0.08685001814505677, 0.07731189614652889], [0, 0.7777777777777778, 0.5673758865248227, 0.20233960884664595, 0.150249966220781, 0.09047029390210003, 0.06183540344920025, 0.05402054185868689, 0.0453869764846148, 0.03974476511789945], [0, 0, 0, 0, 0.5667311411992263, 0.29082426127527217, 0.15990202939118264, 0.14401720892713096, 0.12671172906524708, 0.0950269648071963]]\n",
    "my_dates = dates.date2num(traversed_dates)\n",
    "plt.figure(figsize=(11.5,4.5))\n",
    "plt.xlabel('Date id', fontsize=12, fontweight = 'bold')\n",
    "plt.ylabel('TRR', fontsize=12, fontweight = 'bold')\n",
    "plt.title('Area TRR rates', fontsize=14, fontweight=\"bold\")\n",
    "plt.plot_date(my_dates, four_areas_ttr2[0], ls = \"solid\", label = 'Harrow')\n",
    "plt.plot_date(my_dates, four_areas_ttr2[1], ls = \"solid\", label = 'Redbridge')\n",
    "plt.plot_date(my_dates, four_areas_ttr2[2], ls = \"solid\", label = 'Richmond upon Thames')\n",
    "plt.plot_date(my_dates, four_areas_ttr2[3], ls = \"solid\", label = 'Southwark')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> NMF for 4 areas </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods = ['Harrow', 'Redbridge', 'Richmond upon Thames', 'Southwark']\n",
    "for neigh in neighbourhoods:\n",
    "    just_reviews_series = generate_string(neigh)\n",
    "    topic_table = nmf_2(just_reviews_series.values.astype('U'))\n",
    "    print(topic_table.get_string(title=neigh))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
